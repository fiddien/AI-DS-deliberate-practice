This is my note while watching [Deep Learning on Graphs: Successes, Challenges, and Next Steps](https://www.youtube.com/watch?v=PLGcx65MhCc).

### Success

Inductive bias is the set of assumptions that the learner uses to predict outputs given inputs that it has not encounteres

Multilayer perceptrons: Universal approximators

The problem:
- needs to see a lot of data to learn invariance
	- example: invariance in translated image

Shared local weights
- CNN
- recycle the same weights and apply them to different sections of the data (images)

Let's look at a molecule
- energy U?
- turn the nodes into vectors: problems!

![](attachments/Pasted%20image%2020211208111655.png)

Deep learning on graphs: finding the right inductive biases for some problems

Relational inductive biases

History of GDL

What makes Graph Neural Networks interesting?

Grid
- Constant number of neighbors
- Fixed ordering of neightbors

Graph
- Different number of neighbors
- No ordering of neighbors

GNN Blueprint
- Update + Aggregate
	- Permutation-invariant AGG
		- ChebNet, GCN
		- MoNet, GAT


### Challenges

The trend
- AlexNet (2012), relatively shallow (8 Layers), large filters (11x11)
- VGG19 (2014), deep (20 layers), small filters (3x3)
	- CNN: build complex features out of simple ones (compositionality)

GNN are equivalent to Wisfeiler-Lehman test
- a graph coloring algorithm that cannot count triangles

Depth is rather problematic in GNN: Information bottleneck
- exponential growth of neighbors + long range information = bottleneck

Graph Substructure Network (GSN)
- count pre-defined k-size substructure of each node
- strictly more powerful than Wisfeiler-Lehman
- problem-specific inductive bias
	- triangles in social networks
	- cycles in molecule networks

### Next Steps

Data + Compute + Software

The road ahead
- Standardized benchmark ("ImageNet for graphs")
	- Graph problems are more varied than others
	- Open Graph Benchmark
		- Graph classification
		- Node classification
		- Link prediction
- Software libraries
	- DeepGraphLibrary
	- Spektral
	- PyTorch Geometric
- Efficiency and Scalability
	- The assumptions of statistical independence in stochastic optimization sometimes does not apply to graphs
- Dynamic graphs
	- Real worlds such as Twitter are continous-time dynamic
	- Temporal Graph Networks
- Higher order structures
- Latent graphs ("learning the graph")
	- Build the graph as the part of the learning process
		- General aggregation
		- kNN graph constructed on the fly
		- Graph updated between layers (Dynamic Graph CNN)
		- Task-dependent graph
		- Separate input graph from computational graph
	- Manifold learning 2.0
		- Manifold learning 1.0
			1. Represent data structure as a graph
			2. Compute low-dimensional embedding
			3. Apply ML
		- The new approach: represent data structure as a graph, apply ML directly on the graph
- Theory, robustness, performance guarantees
- "Killer app"
	- recommender system
	- neutrino detection
	- LHC
	- fake news detection
	- drug repurposing
	- chemistry

Computational chemistry and drug design
- the amount of possible synthesizable molecules is huge
- computational funnel

Combinatorial drug therapy (Zitnik et al 2018)

 GNN
 - scene relation
 - few shot learning